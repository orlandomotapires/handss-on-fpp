{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f1ef18",
   "metadata": {},
   "source": [
    "# Hands-On 3: Parallelization with MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c24d6e",
   "metadata": {},
   "source": [
    "Welcome to Hands-on _Parallelization with MPI_. This Hands-on comprises 3 sessions. Next table shows the documents and\n",
    "files needed to develop each one of the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d9803-6dee-44aa-8bb1-ce51765b0616",
   "metadata": {},
   "source": [
    "|  Sessions     | Codes               | files              | \n",
    "| --------------| --------------------| ------------------ |\n",
    "| Session 1     |  Basic Operations   |   operations.c   | \n",
    "| Session 2     | Algebraic Function  |  function.c      | \n",
    "| Session 3     |  Tridiagonal Matrix |   tridiagonal.c  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674a951",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `Basic Operations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72903bc-7fd4-4e56-8281-d3e3986d9d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./material/operations.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./material/operations.c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h> \n",
    "#define SIZE 12\n",
    "\n",
    "int main (int argc, char **argv){\n",
    "  int i, sum = 0, subtraction = 0, mult = 1;\n",
    "  int array[SIZE];\n",
    "\n",
    "  char operations[] = {'+', '-', '*'};\n",
    "  char operationsRec;\n",
    "  int numberOfProcessors, id, to, from, tag = 1000;\n",
    "  int result, value;\n",
    "  \n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcessors);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
    "  MPI_Status status;\n",
    "\n",
    "  switch(id){\n",
    "    case 0: //Master\n",
    "      for(i = 0; i < SIZE; i++){\n",
    "        array[i] = i + 1;\n",
    "        printf(\"%d: %d\\t\", i, array[i]);\n",
    "      }\n",
    "\n",
    "      printf(\"\\n\");\n",
    "      for(to = 1; to < numberOfProcessors; to++) {\n",
    "        MPI_Send(&array, SIZE, MPI_INT, to, tag, MPI_COMM_WORLD);\n",
    "        MPI_Send(&operations[to-1], 1, MPI_CHAR, to, tag, MPI_COMM_WORLD);\n",
    "      }\n",
    "      break;\n",
    "    default: // Workers\n",
    "      MPI_Recv(&array, SIZE, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);\n",
    "      MPI_Recv(&operationsRec, 1, MPI_CHAR, 0, tag, MPI_COMM_WORLD, &status);\n",
    "      switch (operationsRec) {\n",
    "        case '+': // Worker 1\n",
    "          value = 0; \n",
    "          for(i = 0; i < SIZE; i++)\n",
    "          value += array[i];\n",
    "          break;\n",
    "        case '-': // Worker 2\n",
    "          value = 0;\n",
    "          for(i = 0; i < SIZE; i++)\n",
    "          value -= array[i];\n",
    "          break;\n",
    "        case '*': // Worker 3\n",
    "          value = 1;\n",
    "          for(i = 0; i < SIZE; i++)\n",
    "          value *= array[i];\n",
    "          break;\n",
    "      }\n",
    "  }\n",
    "\n",
    "  MPI_Send(&value, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);\n",
    "  MPI_Send(&operationsRec, 1, MPI_CHAR, 0, tag, MPI_COMM_WORLD);\n",
    "\n",
    "  for(to = 1; to < numberOfProcessors; to++) {\n",
    "    MPI_Recv(&result, 1, MPI_INT, to, tag, MPI_COMM_WORLD, &status);\n",
    "    MPI_Recv(&operationsRec, 1, MPI_CHAR, to, tag, MPI_COMM_WORLD, &status);\n",
    "    printf (\"(%c) = %d\\n\", operationsRec, result);\n",
    "  }\n",
    "  \n",
    "  // Validating the results\n",
    "  for(i = 0; i < SIZE; i++) \n",
    "  array[i] = i + 1;\n",
    "\n",
    "  for(i = 0; i < SIZE; i++)\n",
    "    printf(\"array[%d] = %d\\n\", i, array[i]);\n",
    "\n",
    "  for(i = 0; i < SIZE; i++) \n",
    "  {\n",
    "    sum = sum + array[i];\n",
    "    subtraction = subtraction - array[i]; \n",
    "    mult = mult * array[i];\n",
    "  }\n",
    "\n",
    "  printf(\"Sum = %d\\n\", sum); \n",
    "  printf(\"Subtraction = %d\\n\", subtraction); \n",
    "  printf(\"Multiply = %d\\n\", mult);\n",
    "\n",
    "  return 0;\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f9fac-4e5d-4d0b-9e48-99e7181f8a73",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77d58de-a897-4ee6-b5b6-fa0be55cfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc ./material/operations.c -o operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77896ee-490c-4c9a-9d32-22c73d9dd469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1\t1: 2\t2: 3\t3: 4\t4: 5\t5: 6\t6: 7\t7: 8\t8: 9\t9: 10\t10: 11\t11: 12\t\n",
      "(+) = 78\n",
      "(-) = -78\n",
      "(*) = 479001600\n",
      "array[0] = 1\n",
      "array[1] = 2\n",
      "array[2] = 3\n",
      "array[3] = 4\n",
      "array[4] = 5\n",
      "array[5] = 6\n",
      "array[6] = 7\n",
      "array[7] = 8\n",
      "array[8] = 9\n",
      "array[9] = 10\n",
      "array[10] = 11\n",
      "array[11] = 12\n",
      "Sum = 78\n",
      "Subtraction = -78\n",
      "Multiply = 479001600\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 ./operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be567e12",
   "metadata": {},
   "source": [
    "## `Algebraic Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe75ce",
   "metadata": {},
   "source": [
    "The idea of this Hands-on is to make an algorithm that uses the\n",
    "`MPI_Recv` and `MPI_Send` routines in the Master-Worker Paradigm in such\n",
    "a way that in the sequential code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f91e78-8c2a-4ce1-8d6b-90a50ca1401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./material/function.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./material/function.c\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main (int argc, char **argv){\n",
    "    double coef[4], result[4], x = 10, total, received_result;   \n",
    "    int tag = 1000, numberOfProcessors, id, to, from, i, id_indexed, received_index;\n",
    "\n",
    "    for(i = 1; i <= 4; i++) coef[i-1] = i;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcessors);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
    "    MPI_Status status;\n",
    "\n",
    "    switch(id){\n",
    "        case 0: //Master\n",
    "            for(to = 1; to < numberOfProcessors; to++) {\n",
    "                MPI_Send(&x, 1, MPI_DOUBLE, to, tag, MPI_COMM_WORLD);\n",
    "            }\n",
    "            break;\n",
    "\n",
    "        default: // Workers\n",
    "            MPI_Recv(&x, 1, MPI_DOUBLE, 0, tag, MPI_COMM_WORLD, &status);\n",
    "            \n",
    "            id_indexed = 3 - id;\n",
    "            result[id_indexed] = coef[id_indexed];\n",
    "            for(i = 1; i <= id; i++)\n",
    "                result[id_indexed] *= x;\n",
    "            break;\n",
    "    }\n",
    "\n",
    "    MPI_Send(&result[id_indexed], 1, MPI_DOUBLE, 0, tag, MPI_COMM_WORLD);\n",
    "    MPI_Send(&id_indexed, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);\n",
    "\n",
    "    if(id == 0) {\n",
    "        for(from = 1; from < numberOfProcessors; from++) {\n",
    "            MPI_Recv(&received_result, 1, MPI_DOUBLE, from, tag, MPI_COMM_WORLD, &status);\n",
    "            MPI_Recv(&received_index, 1, MPI_INT, from, tag, MPI_COMM_WORLD, &status);\n",
    "            result[received_index] = received_result;\n",
    "            printf (\"(%d) = %lf\\n\", received_index, result[received_index]);\n",
    "            total += received_result;\n",
    "        }\n",
    "\n",
    "        if(total > 0) printf(\"Total: %.5lf\\n\", total);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf967504-a3fd-48af-8f06-f1806b156548",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf9f267-0e70-45f0-9353-cf632a478cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc ./material/function.c -o ./output/function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1805d761-a421-4783-87c0-61ca5f134ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) = 30.000000\n",
      "(1) = 200.000000\n",
      "(0) = 1000.000000\n",
      "Total: 1230.00000\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 ./output/function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c283708-51ca-42ff-b74b-0bfa30520332",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `Tridiagonal Matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f34242-b01c-4a4d-a20c-3b285fd2f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./material/tridiagonal.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./material/tridiagonal.c\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "#define ORDER 4\n",
    "\n",
    "void printMatrix (int m[][ORDER]) \n",
    "{\n",
    "  int i, j;\n",
    "  for(i = 0; i < ORDER; i++) {\n",
    "    printf (\"| \");\n",
    "    for (j = 0; j < ORDER; j++) {\n",
    "      printf (\"%3d \", m[i][j]);\n",
    "    }\n",
    "    printf (\"|\\n\");\n",
    "  }\n",
    "  printf (\"\\n\");\n",
    "}\n",
    "\n",
    "int main (int argc, char **argv){\n",
    "  int k[3] = {100, 200, 300};\n",
    "  int matrix[ORDER][ORDER] = {0}, received_matrix[ORDER][ORDER], i, j;\n",
    "  \n",
    "  int tag = 1000, numberOfProcessors, id;\n",
    "\n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcessors);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
    "  MPI_Status status;\n",
    "\n",
    "  if(id == 0) {\n",
    "    for(int to = 1; to < numberOfProcessors; to++) {\n",
    "      MPI_Send(&matrix, ORDER * ORDER, MPI_INT, to, tag, MPI_COMM_WORLD);\n",
    "    }\n",
    "    for(i = 1; i < numberOfProcessors; i++){\n",
    "      MPI_Recv(&received_matrix, ORDER * ORDER, MPI_INT, MPI_ANY_SOURCE, tag, MPI_COMM_WORLD, &status);\n",
    "      int worker_id = status.MPI_SOURCE;\n",
    "      switch(worker_id){\n",
    "        case 1:\n",
    "          for(j = 0; j < ORDER; j++) matrix[j][j]     += received_matrix[j][j];    //main diagonal           \n",
    "          break;\n",
    "        case 2:\n",
    "          for(j = 0; j < ORDER; j++) {\n",
    "            matrix[j + 1][j] += received_matrix[j + 1][j];    //subdiagonal\n",
    "            matrix[j][j + 1] += received_matrix[j][j + 1];    //superdiagonal\n",
    "          }\n",
    "          break;\n",
    "        case 3:\n",
    "          for(j = 0; j < ORDER; j++) {\n",
    "            if(!(j == (ORDER - 1))) matrix[j][j + 1] += received_matrix[j][j + 1];    //superdiagonal\n",
    "            if(!(j == 0)) matrix[j][j - 1] += received_matrix[j][j - 1];    //subdiagonal\n",
    "          }\n",
    "          break;\n",
    "      }\n",
    "    }\n",
    "  } else {\n",
    "    MPI_Recv(&matrix, ORDER * ORDER, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);\n",
    "    switch(id){\n",
    "      case 1:\n",
    "        for(i = 0; i < ORDER; i++)\n",
    "          for(j = 0; j < ORDER; j++) if( i == j ) matrix[i][j] = i + j + 1 + k[0];             \n",
    "        break;\n",
    "      case 2:\n",
    "        for(i = 0; i < ORDER; i++)\n",
    "          for(j = 0; j < ORDER; j++) if(i == (j + 1)){\n",
    "            matrix[i][j] = i +  j + 1 + k[1];\n",
    "            matrix[j][i] = matrix[i][j] + k[2];\n",
    "          }\n",
    "        break;\n",
    "      case 3:\n",
    "        for(i = 0; i < ORDER; i++)\n",
    "          for(j = 0; j < ORDER; j++) if(!(i == j) && !(i == (j + 1))) matrix[i][j] = 0;\n",
    "        break;\n",
    "    }\n",
    "    MPI_Send(&matrix, ORDER * ORDER, MPI_INT, 0, tag, MPI_COMM_WORLD);\n",
    "  }\n",
    "\n",
    "  printf(\"ID: %d\\n\", id);\n",
    "  printMatrix(matrix);\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d8529-8213-446f-a518-e2f6a6124aba",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb67c90-8caa-4f42-9ec5-174c5add5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc ./material/tridiagonal.c -o ./output/tridiagonal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e01571-7a54-4eab-87b7-0b49e539e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1\n",
      "| 101   0   0   0 |\n",
      "|   0 103   0   0 |\n",
      "|   0   0 105   0 |\n",
      "|   0   0   0 107 |\n",
      "\n",
      "ID: 2\n",
      "|   0 502   0   0 |\n",
      "| 202   0 504   0 |\n",
      "|   0 204   0 506 |\n",
      "|   0   0 206   0 |\n",
      "\n",
      "ID: 3\n",
      "|   0   0   0   0 |\n",
      "|   0   0   0   0 |\n",
      "|   0   0   0   0 |\n",
      "|   0   0   0   0 |\n",
      "\n",
      "ID: 0\n",
      "| 101 502   0   0 |\n",
      "| 202 103 504   0 |\n",
      "|   0 204 105 506 |\n",
      "|   0   0 206 107 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 ./output/tridiagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad055c0c-b24d-4d0f-8b8c-3093b0c601b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdf090-9dd3-491b-947f-81af7b8af818",
   "metadata": {},
   "source": [
    "M. Boratto. Hands-On Supercomputing with Parallel Computing. Available: https://github.com/muriloboratto/Hands-On-Supercomputing-with-Parallel-Computing. 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02e6bc-84d6-44d4-bb66-2a98941d08d0",
   "metadata": {},
   "source": [
    "B. Chapman, G. Jost and R. Pas. Using OpenMP: Portable Shared Memory Parallel Programming. The MIT Press, 2007, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
